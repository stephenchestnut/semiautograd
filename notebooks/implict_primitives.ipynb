{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b17d16-93f3-44df-8b97-ff9b5a88b8e9",
   "metadata": {},
   "source": [
    "# Implicit differentiation\n",
    "\n",
    "The other notebook was all about defining primatives.  For each of those primatives, we had some kind of basic function we wanted to add to the library and we knew the functional form of its derivative.  Adding the primative was just an exercise in writing that all down in code.  The cosine example showed us a few potential benefits to creating primatives instead of using existing ones.\n",
    "\n",
    "* Smaller computational graph means less memory.\n",
    "* Option to pull in code that's not compatible with automatic differentiation.\n",
    "* Numerical stability.\n",
    "\n",
    "Sometimes, we can't just write down the derivative because the function is mess but we can use a trick called implicit differentiation to solve for the derivative.  That's what this notebook is about.  We're going two cover two variaties of \"Implicit Layers\" as the creators call them - fixed-point layers and optimization layers.  Neural ODEs is another use for this technology but we won't cover that here.\n",
    "\n",
    "One defining feature of both fixed-point layers and optimization layer is that the output of the function is also defined implicitly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c979732-61d3-4bed-a5d8-0355235e846d",
   "metadata": {},
   "source": [
    "## Fixed-point layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f21ec77d-d7bc-43c3-873b-ff3e60e52a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Any\n",
    "from semiautograd import Scalar, Function, trace, backward, reset_grad\n",
    "from semiautograd import Abs, Cos, Plus, Times, Sum, Pow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc76cc-1a7f-40e8-8bab-6eb7026675ec",
   "metadata": {},
   "source": [
    "First up is a fixed-point calculation.  Suppose we want to differentiate the solution to a fixed-point problem with respect to some other data in the problem.  For this example we'll look for a fixed point\n",
    "$$x = \\cos(a x + b)$$\n",
    "and the question is, what are $d x/d a$ and $d x/d b$?.\n",
    "\n",
    "Well, cosine is differentiable and it's a nice enough function that we can find the fixed point just by iteratively applying the function.   We already have cosine available in semiautograd, so it's easy to code up the fixed point iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a04f2c3-d6a8-4677-9c3d-7dd5dea960ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=0.7390851699445545 = Cos(0.7390850786891229)\n",
      "len(trace(x))=126\n",
      "a.grad=-0.2974721293726153, b.grad=-0.40248894024267656\n",
      "\n",
      "x=1.0 = Cos(1.199040866595169e-14)\n",
      "len(trace(x))=18\n",
      "a.grad=-1.1990409980613424e-14, b.grad=-1.1990409980622253e-14\n"
     ]
    }
   ],
   "source": [
    "def iterativefixedpoint(*args, fun=None, x0=0):\n",
    "    ''' Find a fixed point x = fun(x,*args) by iterating until convergence\n",
    "\n",
    "    Arguments:\n",
    "        args -- List[Scalar] or List[float] arguments to fun\n",
    "        fun -- the function\n",
    "        x0 -- initial guess\n",
    "    Returns:\n",
    "        the fixed point, either as a Scalar or as a float\n",
    "    '''\n",
    "    eps = 1e-7\n",
    "    oldx = Scalar(x0-1)\n",
    "    x = Scalar(x0)\n",
    "    isscalar = isinstance(args[0],Scalar)\n",
    "    args = [a if isinstance(a,Scalar) else Scalar(a) for a in args]\n",
    "    ii=0\n",
    "    while ii<100 and Abs( Plus(x, Times(Scalar(-1), oldx)))>eps: # | x - oldx |\n",
    "        oldx = x\n",
    "        x = fun(x,*args)\n",
    "        ii += 1\n",
    "    if ii==100:\n",
    "        print(\"Failed to converge\")\n",
    "    if isscalar:\n",
    "        return x\n",
    "    return x.value\n",
    "\n",
    "def cosaxb(x, a, b):\n",
    "    return Cos(Plus(b, Times(a,x)))\n",
    "    \n",
    "a = Scalar(1)\n",
    "b = Scalar(0)\n",
    "x = iterativefixedpoint(a, b, fun=cosaxb)\n",
    "print(f'{x=}')\n",
    "print(f'{len(trace(x))=}')\n",
    "backward(x)\n",
    "print(f'{a.grad=}, {b.grad=}')\n",
    "\n",
    "print('')\n",
    "\n",
    "a = Scalar(-0.5)\n",
    "b = Scalar(0.5)\n",
    "x = iterativefixedpoint(a, b, fun=cosaxb)\n",
    "print(f'{x=}')\n",
    "print(f'{len(trace(x))=}')\n",
    "backward(x)\n",
    "print(f'{a.grad=}, {b.grad=}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8c8249-abf4-4bb1-9e5b-f81ea44a54a7",
   "metadata": {},
   "source": [
    "The answers are right but 126 nodes in that trace is a lot.  Trying to solve for x and then differentiating in order to create a primative is probably hopeless.  Here comes implicit differentiation with $f(x,a,b)=\\cos(a x + b)$,\n",
    "$$\\partial x/\\partial a = df(x(a),a,b)/da = \\partial f(x,a,b) / \\partial a + (\\partial f(x,a,b) / \\partial x) \\times (\\partial x/\\partial a).$$\n",
    "Collecting the terms above we get\n",
    "$$\\partial x/\\partial a = \\frac{\\partial f(x,a,b) / \\partial a}{1-\\partial f(x,a,b)/\\partial x}.$$\n",
    "\n",
    "There's no reason to care how we computed the fixed-point x, implicit differentiation just magic-ed it's derivative into existence from the solution.  While we could actually work this out for $\\cos(a x + b)$, that would kind of limit our fixed-point finding to functions with easy derivatives.  If only we had a generic way to compute the partial derivatives on the right hand side above, oh snap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03f18a5d-60cd-4152-9c33-1513db194f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trace(x)=[0.7390851699445545 = FixedPoint(1,0,fun=<function cosaxb at 0x1107ae980>), 0, 1]\n",
      "a.grad=-0.29747436345601425, b.grad=-0.402489964016367\n"
     ]
    }
   ],
   "source": [
    "def fpbackward(*args, fun=None, x0=0):\n",
    "    x = fpforward(*args,fun=fun, x0=x0) #recompute forward b/c we didn't store it\n",
    "    sargs = [Scalar(a) for a in args]\n",
    "    sx = Scalar(x)\n",
    "    v = fun(sx,*sargs)  # Put the solution into the function\n",
    "    backward(v)         # compute the partial derivatives\n",
    "    return [sa.grad / (1-sx.grad) for sa in sargs]\n",
    "\n",
    "\n",
    "FixedPoint = Function(\"FixedPoint\", iterativefixedpoint, fpbackward)\n",
    "\n",
    "a = Scalar(1)\n",
    "b = Scalar(0)\n",
    "x = FixedPoint(a,b,fun=cosaxb)\n",
    "print(f'{trace(x)=}')\n",
    "backward(x)\n",
    "print(f'{a.grad=}, {b.grad=}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290499f8-c9d6-4e0a-8af9-f410f5bd4eff",
   "metadata": {},
   "source": [
    "## Optimization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea87d9e7-7142-49ec-ae39-8ffede429a71",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 2) (2830798651.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    f'(a,b,x) = 0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
     ]
    }
   ],
   "source": [
    "df(a,b,x) / dx = 0\n",
    "f'(a,b,x) = 0\n",
    "df'(a,b,x)/da = 0 = df'/da + df'/dx dx/da$#\n",
    "dx/da = -df'/da / df'/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a9ace9-0eb8-4974-9523-10e7c7236e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimizeforward(*args, fun=None, dfundx=None, x0=0, lr=0.1, max_iter=1000):\n",
    "    x = x0\n",
    "    ii=0\n",
    "    notconverged = True\n",
    "    eps = 1e-3\n",
    "    while (ii<max_iter) and notconverged:\n",
    "        ii+=1\n",
    "        s_args = [Scalar(a) for a in args]\n",
    "        s_x = Scalar(x)\n",
    "        y = fun(s_x, *s_args)\n",
    "        backward(y)\n",
    "        x += -lr * s_x.grad\n",
    "        notconverged = abs(s_x.grad)>eps\n",
    "    if notconverged:\n",
    "        print('minimizeforward Failed to converge')\n",
    "    return x\n",
    "\n",
    "def minimizebackward(*args, fun=None, dfundx=None, x0=0, lr=0.1, max_iter=1000):\n",
    "    x = Scalar(minimizeforward(*args, fun=fun, dfundx=dfundx, x0=x0, lr=lr, max_iter=max_iter))\n",
    "    s_args = [Scalar(a) for a in args]\n",
    "    y = dfundx(x,*s_args)\n",
    "    backward(y)\n",
    "    if abs(x.grad)<1e-3:\n",
    "        print('minimizebackward does not support functions with d^2f/dx^2 = 0')\n",
    "    return [-a.grad / x.grad for a in s_args] # requires dfundx(x)!=0\n",
    "    \n",
    "def quadratic(x,a,b):\n",
    "    return Sum(Times(a, Pow(x,p=2)), Times(b, x))\n",
    "\n",
    "def dquadraticdx(x,a,b):\n",
    "    return Sum(Times(a, Times(x,Scalar(2))), b)\n",
    "    \n",
    "minimizeforward(1,0,fun=quadratic,dfundx=None,x0=0)\n",
    "Minimize = Function(\"Minimize\", minimizeforward, minimizebackward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357b6a84-15d9-4359-8caf-b6048fab3a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003777893186295717 = Minimize(1,0,fun=<function quadratic at 0x1107ae3e0>,dfundx=<function dquadraticdx at 0x1107ae520>,x0=0.1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0003777893186295717 = Minimize(1,0,fun=<function quadratic at 0x1107ae3e0>,dfundx=<function dquadraticdx at 0x1107ae520>,x0=0.1) <grad=1>,\n",
       " 0 <grad=-0.5>,\n",
       " 1 <grad=-0.0003777893186295717>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = Scalar(1)\n",
    "b = Scalar(0)\n",
    "x = Minimize(a,b,fun=quadratic,dfundx=dquadraticdx, x0=0.1)\n",
    "print(x)\n",
    "backward(x)\n",
    "display(trace(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f695df85-32ce-4d16-81c2-bd0222f10d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2501269499458355 = Minimize(2,-1,fun=<function quadratic at 0x1107ae3e0>,dfundx=<function dquadraticdx at 0x1107ae520>,x0=1,lr=0.1,max_iter=1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2501269499458355 = Minimize(2,-1,fun=<function quadratic at 0x1107ae3e0>,dfundx=<function dquadraticdx at 0x1107ae520>,x0=1,lr=0.1,max_iter=1000) <grad=1>,\n",
       " -1 <grad=-0.25>,\n",
       " 2 <grad=-0.12506347497291775>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = Scalar(2)\n",
    "b = Scalar(-1)\n",
    "x = Minimize(a,b,fun=quadratic,dfundx=dquadraticdx, x0=1, lr=0.1, max_iter=1000)\n",
    "print(x)\n",
    "backward(x)\n",
    "display(trace(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f8baf2c-2d01-4e07-bf92-43ef7b58c085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=2 <grad=-0.12506347497291775>\n",
      "ahat=1.4395180480454486, loss.value=0.23002584979209342\n",
      "ahat=1.3847254217289886, loss.value=0.16013589480537632\n",
      "ahat=1.3370430170652248, loss.value=0.10384281388725108\n",
      "ahat=1.2974875261161845, loss.value=0.06211924241381286\n",
      "ahat=1.2663739460182275, loss.value=0.034085922552572985\n",
      "ahat=1.243223601699386, loss.value=0.017124219105256723\n",
      "ahat=1.2268526206239796, loss.value=0.00795449741883351\n",
      "ahat=1.215784367648377, loss.value=0.0034482780813583735\n",
      "ahat=1.2085437869680224, loss.value=0.0014232901298072355\n",
      "ahat=1.2039325766734423, loss.value=0.0005636729709401966\n",
      "ahat=1.2010473682162583, loss.value=0.0002173408884092487\n",
      "ahat=1.1992562251553487, loss.value=8.298661999315701e-05\n",
      "ahat=1.1981579337378985, loss.value=3.1022288393772386e-05\n",
      "ahat=1.1974860934117495, loss.value=1.156886497544171e-05\n",
      "ahat=1.1970742345874161, loss.value=4.345082705508395e-06\n",
      "ahat=1.1968236200428806, loss.value=1.6081443312819469e-06\n",
      "ahat=1.1966708876421972, loss.value=5.946634533073333e-07\n",
      "ahat=1.1965775757912298, loss.value=2.218511490933145e-07\n",
      "ahat=1.1965205890394925, loss.value=8.271817743727845e-08\n",
      "ahat=1.1964857947501073, loss.value=3.083093458458155e-08\n",
      "ahat=1.196464553547937, loss.value=1.148890444380024e-08\n",
      "ahat=1.196451587376161, loss.value=4.2806848630942145e-09\n",
      "ahat=1.196443672921837, loss.value=1.5948245729051677e-09\n",
      "ahat=1.1964388421574732, loss.value=5.941433312650865e-10\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(314)\n",
    "truea = 1 + random.random()\n",
    "print(f'{a=}')\n",
    "N = 100\n",
    "bs = [2*random.random()-1 for _ in range(N)]\n",
    "xs = [minimizeforward(truea,b,fun=quadratic) for b in bs]\n",
    "\n",
    "ahat = 1.5\n",
    "notconverged = True\n",
    "ii = 0 \n",
    "lr = 0.05\n",
    "while ii<1000 and notconverged:\n",
    "    sa = Scalar(ahat)\n",
    "    sbs = [Scalar(b) for b in bs]\n",
    "    sxs = [Scalar(x) for x in xs]\n",
    "    xhats = [Minimize(sa,sb,fun=quadratic,dfundx=dquadraticdx, x0=0, lr=0.1, max_iter=1000) for sb in sbs]\n",
    "    loss = Sum(*[Pow(Plus(xhat,Times(Scalar(-1),Scalar(x))),p=2) for xhat,x in zip(xhats,xs)])\n",
    "    backward(loss)\n",
    "    ahat += -lr * sa.grad\n",
    "    print(f'{ahat=}, {loss.value=}')\n",
    "    if abs(sa.grad)<1e-4:\n",
    "        notconverged=False\n",
    "    ii+=1\n",
    "if notconverged:\n",
    "    print(\"Failed to converge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda83f3-211c-411e-886a-f62755279d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
